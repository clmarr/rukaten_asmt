{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rukaten task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made this notebook more or less as a walkthrough on how I would tackle this problem. \n",
    "I have stored non-interesting auxiliary functions in UTILS.py so as not to clutter the notebook. \n",
    "\n",
    "First, we import the necessary packages for us to work on the data. I've made a bash script to do that, so we just call it with subprocess. After this, we extract the training data in preparation for the following initial analysis.\n",
    "\n",
    "In the end, building the model took longer than I thought it would. Prior to actually building it, I was within the three hour mark, but it turned out sklearn's random forest can't handle this data very well, so I switched to pytorch. I think you still get an idea about how I problem solve, even though I prioritized getting a working model over having nice looking visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.call(\"install_requirements.sh\",shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this the GPU? GeForce GTX 1650\n",
      "If not, reset it using reset_gpu()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/k/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = UTILS.get_data(\"rdc-catalog-train.tsv\",d=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also extract the test set data, but we will not be looking at it except for seeing its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = UTILS.get_data(\"rdc-catalog-test.tsv\",d=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before preceding, we want look at the training data from various angles. First, of course, the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'CategoryIdPath']\n"
     ]
    }
   ],
   "source": [
    "# remove the header\n",
    "header = test_data[0]\n",
    "test_data = test_data[1:]\n",
    "print(str(header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set is certainly large enough for us to partition a validation set out of it. If we do so, we hope to be able to stratify it. But we need to see the distribution of categories to see if that is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we clean from the project description, data instances consist of two columns, the first being the input description, and the second being the (output) category. The category appears to be hierarchically designated. Let's get a look at how these categories are distributed in the training set... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequencies by category\n",
    "def get_cat_freqs(set, cat_column = 1):\n",
    "    cat_freqs = {}\n",
    "    for cat in [instance[cat_column] for instance in set]:\n",
    "        if cat in cat_freqs.keys():\n",
    "            cat_freqs[cat] += 1\n",
    "        else:\n",
    "            cat_freqs[cat] = 1\n",
    "    return cat_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3008"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_freqs = get_cat_freqs(train_data)\n",
    "n_cats = len(overall_freqs) # how many unique outputs?\n",
    "n_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.9574468085106"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(list(overall_freqs.values()))\n",
    "#what is the mean freq of each cat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many are \"sparse\"? Let's say sparse is less than 5 occurrences\n",
    "sparse_cats = [k for k in overall_freqs if overall_freqs[k] < 5]\n",
    "len(sparse_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1389627659574468"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as a proportion? \n",
    "len(sparse_cats)/n_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many are *severely* sparse, i.e. unique? \n",
    "len([k for k in overall_freqs if overall_freqs[k] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not too bad. It seems we an stratify our train/validation split based on the output variable, so that we can ensure our validation set is representative. We use sklearn's StratifiedKFold for this, treating the 19 unique outputs as all one type \"UNK\" (for details, consult UTILS.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what we have seen in the writeup, it seems that the \"category\" output is a hierarchical category nesting, with \">\" as the delimiter. Before preceding, we confirm to see if this is true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the maximum \"depth\" if we assume \">\" delimits levels of a category hierarchy?\n",
    "max_depth = max([len(line[1].split(\">\")) for line in train_data+test_data])\n",
    "max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  14  108  865 1573  752  232  148    3]\n"
     ]
    }
   ],
   "source": [
    "print(\"\"+str(UTILS.get_types_per_level(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is still quite plausible that this represents a proper hierarchy; we would assume that many instances simply have a hierarchy that is not deeper than four levels to explain how the numbers taper off after the fourth level. \n",
    "\n",
    "If this is a proper hierarchy starting from the first level, there should be no overlap in subtypes (i.e. the higher level types are disjunct). Now we check that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we make a useful function for testing disjunctiveness between a higher and lower category\n",
    "\n",
    "# @types: list, int, int; @precondition: hi_lvl is less than max_depth -1\n",
    "# @return: True or False\n",
    "def cats_disjunct(data, hi_lvl, lvl_difference = 1):\n",
    "    lo_lvl = hi_lvl + lvl_difference\n",
    "    subset = [l for l in [line[1].split(\">\") for line in data] if len(l) > lo_lvl]\n",
    "    subcats = {}\n",
    "    hi_lvl_types = UTILS.get_unique_types([l[hi_lvl] for l in subset])\n",
    "    lo_lvl_types_left = UTILS.get_unique_types([l[lo_lvl] for l in subset])\n",
    "    for hlt in hi_lvl_types:\n",
    "        subcats[hlt] = UTILS.get_unique_types([l[lo_lvl] for l in subset if l[hi_lvl] == hlt])\n",
    "        for llt in subcats[hlt]:\n",
    "            if llt not in lo_lvl_types_left: #if it has already been hit, that means there is overlap!\n",
    "                return False\n",
    "            lo_lvl_types_left.remove(llt)\n",
    "    return True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does level 0 partition level 1?\t\tTrue\n",
      "Does level 1 partition level 2?\t\tTrue\n",
      "Does level 2 partition level 3?\t\tTrue\n",
      "Does level 3 partition level 4?\t\tTrue\n",
      "Does level 4 partition level 5?\t\tTrue\n",
      "Does level 5 partition level 6?\t\tTrue\n",
      "Does level 6 partition level 7?\t\tTrue\n",
      "Does level 7 partition level 8?\t\tTrue\n"
     ]
    }
   ],
   "source": [
    "# now we only need to go by steps of 1, since disjunctiveness of subcategories is transitive\n",
    "# we can go on the whole set here since we aren't really looking at the test data in this way\n",
    "whole_data = train_data+test_data\n",
    "for i in range(0, max_depth): \n",
    "    print(\"Does level \"+str(i)+\" partition level \"+str(i+1)+\"?\\t\\t\"+str(cats_disjunct(whole_data,i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have now confirmed that our output labels are hierarchical in nature!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a sense of what output labels \"mean\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know our data is hierarchical, but how do the categories branch, and more importantly, can we get a rough sense of how this branching might relate to the textual descriptions? We observe that there are a number of instances with \"Toyota\" in the description..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1337"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyotas = [ln for ln in train_data if \"Toyota\" in ln[0]]\n",
    "len(toyotas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we consider a \"best case scenario\". If \"Toyota\"'s presence uniquely indicates a single categorical nesting, automatically tagging this should be *trivally easy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyota_cats = get_cat_freqs(toyotas)\n",
    "len(toyota_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, clearly we are not so lucky. Actually, this result is quite disturbing, because this means there is *more* variation among descriptions with \"Toyota\" in their description than the dataset as a whole (whole: mean 266 instances per category nesting; Toyotas only: only ~27!) \n",
    "\n",
    "But maybe the categorical nestings associated with \"Toyota\" instances do correspond to a pattern that would be very easy to learn. If, say, Toyota's all fall into only 3 categories at the third nesting, this is also a good scenario for automatic classification as there is a good chance that the rest of the description would suffice for disambiguation. So let's see how many unique categories there are at each level for just descriptions including \"Toyota\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 15 35 36  7]\n"
     ]
    }
   ],
   "source": [
    "print(str(UTILS.get_types_per_level(toyotas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is starting to look *really* bad. It is probable that within \"Toyotas\" there is, instead of any sort of clustering, a data sparsity. But before preceding, let's look at the data itself to see why this might be... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples for head category 4015:\n",
      "For Toyota Corolla Black Manual Remote Replacement Driver Side Mirror (TY2709410-ML00)\n",
      "2-Pack Replacement Engine Air Filter for 2008 Toyota Solara V6 3.3 Car/Automotive\n",
      "3-Pack Replacement Engine Air Filter for 2000 Toyota Sienna V6 3.0 Car/Automotive\n",
      "2-Pack Replacement Engine Air Filter for 2008 Toyota Avalon V6 3.5 Car/Automotive\n",
      "Replacement Cabin Air Filter for 2007 Toyota Corolla L4 1.8 Car/Automotive\n",
      "----\n",
      "\n",
      "Samples for head category 2199:\n",
      "Radiator-1 Row Plastic Tank Aluminum Core CSF 3502 fits 10-11 Toyota Camry\n",
      "2001-2002-2003 Toyota Rav4 Rav-4 Taillight Taillamp Rear Brake Tail Light Lamp Left Driver Side (01 02 03)\n",
      "1989 1990 1991 1992 1993 1994 1995 Toyota 4Runner & Toyota Pickup Truck 4WD (without Antenna Hole) Front Fender Quarter Panel (without Molding Holes) Primed/Steel Left Driver Side (89 90 91 92 93 94 95)\n",
      "For 13-17 Toyota Rav4 4th Gen XA40 Front Bumper Protector Brush Grille Guard (Chrome) 14 15 16\n",
      "TYC 660101 Toyota Matrix Front Passenger Side Replacement Power Window\n",
      "----\n",
      "\n",
      "Samples for head category 3292:\n",
      "2004 2005 2006 Toyota Solara Factory OEM Temperature Control Panel PN 55902AA010\n",
      "Metra Toyota Mini Multi Kit\n",
      "2014 Toyota Camry AM FM Radio CD w Display and Temp Controls 86140-06190 100201\n",
      "Toyota 2002-2004 Camry LE Radio AM FM CD CS w Bluetooth Music 86120-AA040 16823\n",
      "2003-2005 Toyota Ceclia Highlander RAV4 AMFM Radio Cassette CD 86120-2B761 16844\n",
      "----\n",
      "\n",
      "Samples for head category 1395:\n",
      "Toyota FJ Cruiser (1:24 Scale) R/C Car w/Working Headlights (Blue/White)\n",
      "\"Fast & Furious Tunerz Orange Toyota Supra 1:24 Scale R/C 7\"\" Car\"\n",
      "30727 Weld X HPI Racing JZX100 Toyota Mark II Body HPIC0727\n",
      "Lionel Racing Martin Truex Jr Bass Pro 2017 Toyota Camry 1:64 ARC Diecast Car\n",
      "51331 Body Parts Set Toyota Land Cruiser 40 CR-01 TAMC2336 TAMIYA\n",
      "----\n",
      "\n",
      "Samples for head category 3625:\n",
      "Pac Isimple Connect Interface For Android Iphone Ipad Ipod 2003-12 Toyota  9.000000In. X 8.000000In. X 3.000000In.\n",
      "Metra Toyota Camry 2002-2006 Din/ Ddin Dash Kit\n",
      "Winjet 01-07 Toyota Sequoia / 00-06 Toyota Tundra Fog Lights - (Clear) - (Wiring Kit Included)  10.000000In. X 9.500000I\n",
      "Axxess Ax-Tyamp1 Amp Interface For 2001-2015 Lexus(R)/Toyota(R)  9.00In. X 4.15In. X 2.25In.\n",
      "Crux Radio Replacement W/Swc & Oe Rvc Retention For Toyota Vehicles 2012-Up  5.500000In. X 3.000000In. X 11.000000In.\n",
      "----\n",
      "\n",
      "Samples for head category 2075:\n",
      "\"American International TOYK972S Ai 2005-11 Toyota Tacoma \"\"silver\"\"\"\n",
      "METRA 99-8300 2000 - Up Toyota Multi Kit\n",
      "AXXESS AX-TYAMP2-SWC 2012 & Up Toyota(R) Amp Interface with SWC\n",
      "METRA 95-8202 2000 & Up Toyota Scion Double DIN Installation Multi Kit\n",
      "----\n",
      "\n",
      "Samples for head category 4238:\n",
      "ISN PBT71114 Toyota/Lexus Oil Filter Drain Hose\n",
      "K Tool International DYN-6917RX Fender Moulding Clip Red Toyota, Hole Or Size: 17mm, Length Or\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toyota_head_cats = UTILS.get_unique_types([l[1].split(\">\")[0] for l in toyotas]) \n",
    "for thcat in toyota_head_cats:\n",
    "    cat_sample_descs = [l[0] for l in toyotas if l[1].split(\">\")[0]==thcat][:5]\n",
    "    print(\"Samples for head category \"+str(thcat)+\":\\n\"+\"\\n\".join(cat_sample_descs)+\"\\n----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more encouraging, as we can see patterns here:\n",
    "\n",
    "4015: includes air filters and mirrors\n",
    "\n",
    "3292: all include controls that are usually placed near the right hand of the driver\n",
    "\n",
    "1395: appear to be all \"flashy\" products\n",
    "\n",
    "At the same time, there is also a good deal of variance here, so at this point whether automatically classifying this data is feasible remains an open question. Thus, we analyze what would be the \"correct\" model to classify this sort of data, and decide on how best to quickly infer whether that model would be successful, by using a fast and relatively light-weight proxy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What sort of model would be ideal for this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks clearly like a hierarchical classification problem to me.\n",
    "To be honest, I have never done such a problem before. \n",
    "With regards to our evaluation metric, I considered two options.\n",
    "Firstly, I could treat this as a normal classification problem and use an appropriate metric, like F-score (harmonic mean of precision and recall). \n",
    "Alternatively, I could look through the literature and find an appropriate metric, under the assumption that getting the parent or grandparent category right counts for at least something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, Google presented me quickly with what I wanted: a comprehensive literature review on metrics in use in the field of hierarchical text classification. \n",
    "I came across the following two papers,\n",
    "both of which discuss the advantages and disadvantages of different hierarchical classification metrics.\n",
    "Mercifully, all of the discussed metrics are in some way, like F-score, based on precision and recall, usually combining these two with some sense of \"hierarchical distance\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silla 2011: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.183.302&rep=rep1&type=pdf\n",
    "\n",
    "Kosmopoulos 2014:    https://link.springer.com/article/10.1007/s10618-014-0382-x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was looking for the easiest to implement hierarchical similarity metric.\n",
    "In addition, I wanted the one that made the least assumptions about the nature of the hierarchy.\n",
    "After all, I don't even know what the exact categories in this dataset mean as all I can see are their indices in some reference set I don't have access to. \n",
    "\n",
    "The first paper, that of Silla 2011, is widely cited, and I found, still being recommended in ML blogs (like this one: https://towardsdatascience.com/https-medium-com-noa-weiss-the-hitchhikers-guide-to-hierarchical-classification-f8428ea1e076). \n",
    "For a metric that can be broadly applied, they explicitly recommend \"hierarchical F-score\" as formulated by Kiritchenko 2005. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regards to the exact learning model itself, I think this is less important for our purposes.\n",
    "The reason we need to know the evaluation metric we want to use is that this is how we tell if our proxy test is indicating that automatic classification will be worth it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing our proxy tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be honest, I don't really know all I would like to about this dataset.\n",
    "For the output variable, all I have is numbers, which I believe are i ndices to categories on some list -- but perhaps that is by design. \n",
    "So here are some assumptions I am making about the data and the stakeholders in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption 1 -- high level categories matter more\n",
    "I assume that the cost of misclassifying a high level category is more than a low category one.\n",
    "This seems to make sense -- mistaking a Toyota for a Mazda is not ideal, but it is certainly better than mistaking a Toyota for a single Toyota headlight, or even a wine glass or newspaper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption 2 -- it is easier to classify higher level nestings\n",
    "This is almost always the case, and I imagine it would be for taxonomies of products. What this means for our purposes is that if we cannot perform reasonably well at two-level classification, multi-level classification is not remotely worth it. Likewise, single level \"flat\" classification failing will be a signal that two-level classification is not worth it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumption 3 -- Precision and recall are equally important to stakeholders\n",
    "More often than not, this is probably the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our two proposed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 -- can we learn the top level classifications?\n",
    "For this, we first see if we can even classify the highest levels to a decent degree of accuracy, as measured by precision (how many labels we get right), recall (how many we got right for each label) and F-score (the harmonic mean of precision and recall). If using a reasonably powerful but simple model we cannot even tell apart the descriptions of Toyota cars from descriptions of antique doorknobs, we can immediately infer that automatic classification will not be worth it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# remember to make sure gold_labels and pred_labels are not torch tensors of indexes for labs here...!!\n",
    "def F1(gold_labels, pred_labels):\n",
    "    return f1_score(gold_labels, pred_labels, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, per assumption 3, our evaluation metric will be a standard F score, and we hope that for the upper level categories, a trained model will perform at 0.8 or higher. For our classification model, we will use a random forest classifier. To quickly tune relevant training parameters, we will use five fold cross-validation using the validation set, and then train on the whole original train set with our best performing setup before evaluating performance on the held-out test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we cannot achieve a decent performance on this task while using a powerful random forest classifier with tuned parameters, then we can safely say that automatic classification is not worth it. If we can learn this well, we will defer to test 2 for a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 -- can we learn the second level categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform well enough on task 1, we then want to see if a reasonably powerful model can begin to learn the nesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, one would use a model that learns the hierarchical relationships either explicitly (such as a Bayesian Network classifier) or one specifically designed to learn them implicitly. However, for our purposes, we will use the same random forest classifier, retrained on a task to predict the second nested label, or if there is no nesting, the (single) label. In this way, we are indeed implicitly modeling the nesting, since the first nesting perfectly partitions the second nesting. However, our model is not learning to model the relationship between the hierarchical levels, and it is not impossible that in this scenario it could (incorrectly) label an instance X>A when it is actually Y>B and in test 1 for that instance it would have predicted Y. However, if the data is actually agreeable to automatic classification in a way that hierarchies could be learned in the first place, we should be able to achieve greater precision on test 2 than test 1, so the opposite sort of error (in test 1 predicting X, but in test 2 correctly predicting Y>B) should be far more common. If this is not the case, we don't care, because this actually shows us what we wanted to know -- automatically labeling the data is not a good idea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some necessary coding to run task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see below, in terms of how we will stratify the data set for cross-validation, while there are a healthy number of examples for all top level categories in the the training set, the same is not true for second level categories... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies for labels at top level: dict_values([200945, 28412, 268295, 29557, 96714, 23529, 85554, 18847, 8172, 1030, 5648, 8113, 20086, 5098])\n",
      "Frequencies for labels at second level: dict_values([33791, 84438, 12743, 57645, 17424, 61787, 41145, 5408, 2295, 2646, 23713, 23322, 77326, 16161, 3637, 4299, 7491, 75129, 9326, 10610, 8172, 369, 6555, 8490, 8291, 12006, 1947, 5653, 12905, 37879, 2476, 2316, 5683, 1347, 12849, 11564, 10498, 1663, 2315, 1794, 222, 2686, 3358, 633, 943, 5376, 7378, 545, 4708, 1757, 553, 116, 2094, 1007, 5530, 2059, 558, 2776, 706, 387, 1419, 7552, 1977, 747, 1653, 64, 1107, 284, 938, 584, 184, 1086, 193, 309, 1791, 504, 1976, 607, 59, 303, 279, 167, 791, 1015, 556, 944, 937, 282, 730, 98, 72, 207, 269, 175, 24, 498, 104, 98, 28, 55, 329, 34, 171, 82, 62, 84, 24, 47, 1])\n"
     ]
    }
   ],
   "source": [
    "top_level_freqs = get_cat_freqs([['',ln[1].split(\">\")[0]] for ln in train_data])\n",
    "print(\"Frequencies for labels at top level: \"+str(top_level_freqs.values()))\n",
    "second_level_freqs = get_cat_freqs([['',ln[1].split(\">\")[1] if \">\" in ln[1] else ln[1]] for ln in train_data])\n",
    "print(\"Frequencies for labels at second level: \"+str(second_level_freqs.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build a way to recognize the hierarchical relation between the first and second levels anyways for task 2, so this is an opportunity to shoot two birds with one stone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_nesting = {}\n",
    "for (hi, lo)   in [ln[1].split(\">\")[:2] for ln in train_data if \">\" in ln[1]]:\n",
    "    if hi not in first_nesting:\n",
    "        first_nesting[hi] = []\n",
    "    if lo not in first_nesting[hi]:\n",
    "        first_nesting[hi] += [lo]\n",
    "\n",
    "def backoff(lo): #recall there is no overlap\n",
    "    return [k for k,v in first_nesting.items() if lo in v][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we likewise use a neural net that ends in a softmax function, with the same cross-validation process. However, for our evaluation measure will be *Hierarchical F-score* as per Kiritchenko 2005, rather than simple F-score. \n",
    "Furthermore, our target is different. Rather than a mere threshold, our goal is to outperform our final scores on test 1 with statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl1_idx = list(top_level_freqs.keys())\n",
    "lvl2_idx = list(second_level_freqs.keys()) \n",
    "    #these will be useful later.x\n",
    "task2_lab_idx = lvl1_idx + lvl2_idx\n",
    "    # the first |len(lvl1-idx)| indicate cases where there was no second level class in the label\n",
    "len_t2 = len(task2_lab_idx)\n",
    "    \n",
    "def get_parent(lvl2_ind):\n",
    "    return lvl1_idx.index(backoff(lvl2_idx[lvl2_ind-14]))\n",
    "    \n",
    "from torch import sum as tsum\n",
    "\n",
    "# makes onehot vector tensor with 1 at i.\n",
    "def onehot(i,task2=False):\n",
    "    out = torch.zeros(len_t2 if task2 else 14)\n",
    "    out[i] = 1.0\n",
    "    return out\n",
    "\n",
    "# oh -- a onehot vector tensor\n",
    "def onehot_eject(oh):\n",
    "    return oh.tolist().index(1.)\n",
    "\n",
    "# gold_labs -- onehot eject used before passing to this class on gold y originally in form of one hot vector tensor, correct value's index is 1.\n",
    "# pred_labs -- predicted labs, tensor of indices\n",
    "# indexer is task2_lab_idx -- this works for task1 too since we just ignore index 14 onward. \n",
    "def lab_hpr(idx, gold_labs, pred_labs):\n",
    "    this_true = gold_labs == idx\n",
    "    this_pred_correct = this_true*(pred_labs == idx)\n",
    "    if idx < 14 : # no second class in this label, so we are basically doing raw precision.\n",
    "        return float(tsum(this_pred_correct))/float(tsum(this_true))\n",
    "    else:\n",
    "        gold_parents = torch.tensor(\n",
    "            [li if li < 14 \n",
    "             else get_parent(li) for li in gold_labs])\n",
    "        pred_parents = torch.tensor(\n",
    "            [li if li < 14\n",
    "             else get_parent(li) for li in pred_labs])\n",
    "        idx_parent = get_parent(idx)\n",
    "        \n",
    "        parent_true = gold_parents == idx_parent\n",
    "        parent_pred_correct = parent_true*(pred_parents == idx_parent)\n",
    "        numerator = float(tsum(this_pred_correct)+tsum(parent_pred_correct))\n",
    "        denominator = float( tsum(this_true)+tsum(parent_true) )\n",
    "        return numerator/denominator\n",
    "\n",
    "def hP(g,pr): \n",
    "    lab_hPs = torch.tensor([lab_hpr(ci, g, pr)\n",
    "                            for ci in range(0,len_t2)])\n",
    "    gold_lab_rates = torch.tensor([torch.sum(g==ci).float()/len_t2 \n",
    "                                   for ci in range(0,len_t2)])\n",
    "    return torch.sum(lab_hPs * gold_lab_rates).item()\n",
    "    \n",
    "# warning - this will only work if both parameters past are 1d lists/arrays/tensors of int\n",
    "# gold_labs and pred_labs must be tensors of flat ints (indices of labels), not strings or vectors\n",
    "# \"classes\" - list of all possible labels, ints just like above.\n",
    "def task2_hF(gold_labs, pred_labs):\n",
    "    h_precision = hP(gold_labs, pred_labs) \n",
    "    h_recall = hP(pred_labs,gold_labs)                        \n",
    "    numerator = 2.0 * h_precision * h_recall \n",
    "    denominator = h_precision + h_recall\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But... what are our features??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, extracting features from the descriptions is an NLP task. Ideally, we would want to use word embeddings, an attention mechanism, and maybe an LSTM model that takes into account complex sequential relations between tokens. Not a single one of these alone is feasible to do in three hours (at this point of writing I have less than half an hour left -- full disclosure). So instead we extract some basic features. Still, we assert that if a powerful model fails to learn *something* from the following features, automatic classification is not worth it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unigram features\n",
    "The presence of each \"word\" (string token) as a boolean in our description (number of occurrences does not matter). Unigram identity shall not be case sensitive for the first character in a token.\n",
    "\n",
    "### Frequency threshold and unseen tokens in final evaluation\n",
    "There is the issue of sparse tokens when using unigram features. For this reason, we introduce a unigram frequency threshold as a model parameter.. Unigrams with frequency beneath the threshold are transformed into \"UNK\" when calculating unigram features, and do not contribute to classification by the model. Likewise, in prediction stage, any tokens that were unseen in the training set are also immediately transformed into \"UNK\", and thus do not contribute to prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word shape features\n",
    "Word shape concerns what sort of characters appear where in a token. This will help us model the presence of serial numbers, etc. We partition possible characters as follows: uppercase alphabetic, lowercase alphabetic, numeric, and every other unique character as its own type (note that spaces, tabs, and line breaks are used as delimiters and thus not included). \n",
    "\n",
    "We additionally recruit \"word shape summary\" features, which compress unbroken sequences of any 3+ instances of character type x into \"x*\". \n",
    "\n",
    "For obvious reasons, our model will not extract word shape features for tokens consisting of only lowercase alphabetic characters. \n",
    "\n",
    "Like unigram features, merely having one token with the given word shape will make its corresponding feature value positive, regardless of how many times it occurs. Also like our treatment of unigram features, unseen word shapes do not contribute to prediction at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n",
      "nnaaa\n",
      "AAAAA_AAAA!\n",
      "@%#!\n"
     ]
    }
   ],
   "source": [
    "# some examples of how this works \n",
    "print(UTILS.word_shape(\"abc\"))\n",
    "print(UTILS.word_shape(\"12abc\"))\n",
    "print(UTILS.word_shape(\"ANGRY_CAPS!\"))\n",
    "print(UTILS.word_shape(\"@%#!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### examples\n",
    "print(UTILS.ws_summary(\"abc\"))\n",
    "print(UTILS.ws_summary(\"12abc\"))\n",
    "print(UTILS.ws_summary(\"ANGRY_CAPS!\"))\n",
    "print(UTILS.ws_summary(\"@%#!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our automatic label extraction for the first and second levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lv1_lab (ylab):\n",
    "    return ylab.split(\">\")[0]\n",
    "\n",
    "def lv2_lab (ylab):\n",
    "    return ylab.split(\">\")[1] if \">\" in ylab else ylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must set up the experiment. First, we define our model architecture..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_width, output_width):\n",
    "        super(Net,self).__init__()\n",
    "        widths = [input_width,\n",
    "                       min(2048, pow(2, 1+int(math.log(input_width,2)))),\n",
    "                       output_width]\n",
    "        self.logprob = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        contents = OrderedDict(); depth = 2\n",
    "        for i in range(0, depth):\n",
    "            contents['l'+str(i)] = nn.Linear(widths[i], widths[i+1] , bias=False)\n",
    "            if i == 0:\n",
    "                contents['bn'+str(i)] = nn.BatchNorm1d(num_features=widths[i+1])\n",
    "                contents['a'+str(i)] = nn.LeakyReLU(negative_slope=0.07)\n",
    "                contents['d'+str(i)] = nn.Dropout(0.2)\n",
    "        self.network = nn.Sequential(contents)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.network(x)\n",
    "        return out # .squeeze() # this may or may not be necessary to avoid errors.\n",
    "\n",
    "    def predict(self, input):\n",
    "        out = self.forward(x)\n",
    "        # return max probability labels as single ints\n",
    "        return torch.tensor([torch.argmax(yhi) for yhi in out])\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(model_path):\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.load(model_path) if torch.cuda().is_available() else torch.load(model_path, map_location = 'cpu')\n",
    "\n",
    "    def save(self,path):\n",
    "            torch.save(self,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we decide to use cross entropy loss (hence effectively optimizing while using negative log likelihood) as our objective function while learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the training data into a cross-validation set (with 640000 instances -- evenly dividing into minibatches of 1024) and a development set (160000). The development set to halt the final training procedure, while the cv set will be used to determine the optimizer being used and (importantly) the feature threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of cv set 640000\n",
      "size of dev set 160000\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "RS = 42 \n",
    "cv_dev_splitter = StratifiedShuffleSplit(n_splits=1, random_state=RS, test_size = 160000)\n",
    "cv_inds, dev_inds = [] , []\n",
    "y_for_split = np.vectorize(lambda x : lv1_lab(train_data[x][1]))(list(range(len(train_data))))\n",
    "for split in cv_dev_splitter.split(np.zeros_like(y_for_split), y_for_split):\n",
    "    cv_inds, dev_inds = split\n",
    "gc.collect()\n",
    "print(\"size of cv set \"+str(len(cv_inds)))\n",
    "print(\"size of dev set \"+str(len(dev_inds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make our cross-validation procedure use a special case of our train procedure, so we define our training procedure first. Before we do that, we define our batching algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xparser -- a FeatureExtractor instance\n",
    "def batch_iter(batch_size, batch_inds, xparser, yparser, test=False, shuffle=True):\n",
    "    samp_ix = list(range(0,len(test_data))) if test else batch_inds\n",
    "    data_loc = test_data if test else train_data\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(samp_ix)\n",
    "    ix_left = len(samp_ix)\n",
    "    \n",
    "    while ix_left > 0:\n",
    "        j, ysamp, xsamp = 0 , [] , []\n",
    "        while j < batch_size and ix_left > 0:\n",
    "            xsamp += [data_loc[samp_ix[j]][0]]\n",
    "            ysamp += [yparser(data_loc[samp_ix[j]][1])]\n",
    "            j += 1\n",
    "            ix_left -= 1\n",
    "        samp_ix = samp_ix[j:]\n",
    "        xsamp = torch.tensor(np.vectorize(xparser.extract)(xsamp))\n",
    "        ysamp = torch.tensor([onehot(task2_lab_idx.index(yi)) for yi in ysamp])\n",
    "        yield ysamp, xsamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define our evaluation algorithm before we define our training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, eval_inds, xparse, yparse, test = False, batch_size = 256):\n",
    "    model.eval()\n",
    "    \n",
    "    running_avg = 0.0\n",
    "    lines_seen = 0\n",
    "    \n",
    "    for Ys, Xs in batch_idx_iter(batch_size, [] if test else eval_inds, \n",
    "                                 xparse, yparse, test=test):\n",
    "        outputs = model.predict(Xs)\n",
    "        lines_seen += len(outputs)\n",
    "        running_avg = (prev_lines_seen * running_avg + len(outputs) * \n",
    "                       criterion(outputs, Ys ).item()) / lines_seen\n",
    "\n",
    "    model.train()\n",
    "    return running_avg    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our training algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5 # we're impatient\n",
    "patience = 0 #see above. \n",
    "def train(feat_xtr, tr_set, validation_set, opt, \n",
    "          batch_size = 1024, task2 = False, patience = patience,\n",
    "          saveloc=\"last_model.pth\", for_crossval = False):\n",
    "    \n",
    "    final_epoch = 1 if for_crossval else max_epochs\n",
    "    curr_epoch = 0\n",
    "    halt_factor = 1.05 # we're impatient\n",
    "    \n",
    "    best_dev_loss = float('inf')\n",
    "    dev_loss_by_epoch = []\n",
    "    \n",
    "    model = Net(feat_xtr.FT_LEN, len(task2_lab_idx) if task2 else 14)\n",
    "    optimizer = opt(model)\n",
    "    criterion = loss()\n",
    "    yparse = [lv1_lab, lv2_lab][int(task2)] \n",
    "\n",
    "    model.train()\n",
    "    halted = False\n",
    "    \n",
    "    print(\"begin training\")\n",
    "    \n",
    "    while not halted:\n",
    "        print(\"Epoch {} \".format(str(curr_epx)))\n",
    "        curr_batch, print_tot_loss, num_lines = 0 , 0.0 , 0\n",
    "        print(\"batching\")\n",
    "    \n",
    "        for ysmp, xsmp in batch_idx_iter(batch_size, tr_set, feat_xtr, yparse):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outs = model(xsmp)\n",
    "            cur_loss = criterion(outs, ysmp)\n",
    "            \n",
    "            print_tot_loss += cur_loss * len(y) ; num_lines += len(y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            del outs; del cur_loss; del x; del y; gc.collect()\n",
    "            \n",
    "            curr_batch += 1\n",
    "            \n",
    "            if curr_batch % 50 == 0: \n",
    "                print(\"Batch {}, avg loss {}, curr lr {}\".format(curr_batch,print_tot_loss / num_lines, optimizer.param_groups[0]['lr']))\n",
    "                print_tot_loss = num_lines = 0\n",
    "        \n",
    "        if cross_val:\n",
    "            halted = True\n",
    "            model.save(saveloc)\n",
    "            model.train()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                dev_loss_by_epoch += [evaluate(model, validation_set)]\n",
    "    \n",
    "            print(\"dev loss \"+str(dev_loss_by_epoch[-1]))\n",
    "        \n",
    "            halted = halt_factor * dev_loss_by_epoch[-1] > best_dev_loss\n",
    "            if not halted: \n",
    "                best_dev_loss = dev_loss_by_epoch[-1] \n",
    "                curr_epoch += 1\n",
    "            \n",
    "            if patience > 0:\n",
    "                patience -= 1\n",
    "                halted = False\n",
    "                \n",
    "            if dev_loss_by_epoch[-1] < best_dev_loss:\n",
    "                model.save(saveloc)\n",
    "                model.train()\n",
    "            \n",
    "    print(\"done \"+(\"with cv\" if for_crossval else \"training\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll cross validate two optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def opt_SGD(model):\n",
    "    return optim.SGD(model.parameters(), lr=0.0001, momentum=0.95, nesterov=nester, weight_decay=0.0000001)\n",
    "    \n",
    "def opt_Adamax(model):\n",
    "    return optim.Adamax(model.parameters())\n",
    "    \n",
    "optimizer_options = [ opt_SGD, opt_Adamax ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And three options for the occurrence threshold for feature inclusion..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_options = [20,50,160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our cross-validation class as a series of training runs producing a graph of resulting evaluation metrics, among which we choose the most favorable coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train, test in first fold : 512000, 128000\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5 # both the tr and validation in a fold will be divisible by how powers of 2\n",
    "rs = 42\n",
    "# we stratify based on the top level labels -- may be imperfect for lower levels\n",
    "    # but this is only a proxy test. \n",
    "skf = StratifiedKFold(n_splits = n_folds, random_state = rs, shuffle =True)\n",
    "Y_for_cv = np.vectorize(lv1_lab)([train_data[li][1] for li in cv_inds])\n",
    "FOLDS = np.array([f for f in enumerate(skf.split(np.zeros_like(Y_for_cv), Y_for_cv))])\n",
    "print(\"len train, test in first fold : \"+str(len(FOLDS[0][1][0]))+\", \"+str(len(FOLDS[0][1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_eval_f(model, input_inds, xparse, predict_batch_size = 256, task2 = False, test=False):\n",
    "    ypred = []\n",
    "    ygold = []\n",
    "    yparse = [lvl1_lab, lvl2_lab][int(task2)]\n",
    "    ibi = 0\n",
    "    if test:\n",
    "        input_inds = np.array(range(len(test_data)))\n",
    "    \n",
    "    while ibi < len(input_inds):\n",
    "        next_batch_inds = input_inds[ibi*predict_batch_size : (ibi+1)*predict_batch_size]\n",
    "        next_x = torch.tensor([xparse.extract(train_data[ii][0]) for ii in next_batch_inds])\n",
    "\n",
    "        ypred = np.concatenate([ypred, model.predict(next_x).tolist()])\n",
    "        ygold = np.concatenate([ygold, \n",
    "            np.vectorize(lambda x : task2_lab_idx.index(yparse(train_data[x][1])))(next_batch_inds)])\n",
    "        \n",
    "    return([F1, task2_hF][int(task2)](ypred,ygold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# returns tuple (x, y) where\n",
    "    # x -- index of optimal optimization method in optimizer_options\n",
    "    # y -- index of optimal threshold in threshold_options\n",
    "def cross_validate(n_folds_to_use = len(FOLDS), # default is all folds but we can speed it things up\n",
    "                   task2 = False, reuse_paths = True, batch_size = 1024, argbest = np.argmax, \n",
    "                  ignore_bigrams = False):\n",
    "    result_graph = [[0.0 for i in range(0, len(optimizer_options))] for j in range(0, len(threshold_options))]\n",
    "    \n",
    "    for fthri in range(0, len(threshold_options)):\n",
    "        \n",
    "        feat_extractor = UTILS.FeatureExtractor(\n",
    "            build_path = \"feat_extractor_task\"+str(int(task2)+1)+\"_th\"+str(threshold_options[fthri])+\".txt\", \n",
    "            thresh = threshold_options[fthri])\n",
    "        if not feat_extractor.feats_determined:\n",
    "            feat_extractor.build_anew( np.array([l[0] for l in train_data]), ignore_bigrams = ignore_bigrams)\n",
    "        \n",
    "        for opti in range(0, len(optimizer_options)):\n",
    "            scores = []\n",
    "\n",
    "            print(\"cross validating for threshold = \"+str(threshold_options[fthri])+\" with optimizer \"+str(opti))\n",
    "            \n",
    "            for fi in range(0, n_folds_to_use):\n",
    "                \n",
    "                model_path = \"cv_t\"+(\"2\" if task2 else \"1\")+\"_th\"+str(fthri)+\"_opt\"+str(opti)+\".pth\"\n",
    "                if model_path in os.listdir(os.getcwd()) and reuse_paths:\n",
    "                    print(\"Reusing crossval trained model with this config to save time.\")\n",
    "                else: \n",
    "                    print(\"Training model anew...\")\n",
    "                    train(feat_extractor, FOLDS[fi][1][0], [], #train algo does not validate when for_crossval = True\n",
    "                          optimizer_options[opti], batch_size = batch_size, saveloc = model_path, \n",
    "                          for_crossval = True)\n",
    "                \n",
    "                print(\"loading...\")\n",
    "                model = torch.load(model_path) if torch.cuda.is_available() \\\n",
    "                    else torch.load(model_path, map_location='cpu')\n",
    "                \n",
    "                print(\"evaluating...\")\n",
    "                scores += [final_eval_f(model, FOLDS[fi][1][1], feat_extractor, task2 = task2)]\n",
    "                print(\"F = \"+str(scores[-1]))\n",
    "                \n",
    "            result_graph[fthri][opti] = np.mean(scores)\n",
    "        \n",
    "    return np.unravel_index(argbest(result_graph,axis=None), np.array(result_graph).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment class wraps both task1 and task2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment():\n",
    "    \n",
    "    def __init__(self, task2 = False, reuse_path = True):\n",
    "        print(\"Beginning task \"+str(int(task2)+1))\n",
    "        self.task2 = task2\n",
    "        self.reuse_path = reuse_path\n",
    "        self.model_path = \"task\"+str(int(task2)+1)+\".pth\"\n",
    "\n",
    "    def do_cv(self, cv_folds=5, ignore_bigrams = False):\n",
    "        if self.reuse_path and self.model_path in os.listdir(os.getcwd()):\n",
    "            print(\"Reusing model trained with same configurations from earlier, no need to cross-val\")\n",
    "        else:\n",
    "            print(\"Getting cross validation optimizer and feature threshold...\")\n",
    "            self.thi, self.oi = cross_validate(n_folds_to_use = cv_folds, task2 = self.task2, \n",
    "                                               ignore_bigrams = ignore_bigrams)\n",
    "\n",
    "    def extract_features(self):\n",
    "        self.feat_extractor = UTILS.FeatureExtractor(\n",
    "            build_path = \"feat_extractor_task\"+str(int(self.task2)+1)+\"_th\"+str(threshold_options[self.thi])+\".txt\", \n",
    "            thresh = threshold_options[self.thi])\n",
    "        if not self.feat_extractor.feats_determined():\n",
    "            self.feat_extractor.build_anew( np.array([l[0] for l in train_data]))\n",
    "        \n",
    "    def final_train(self):\n",
    "        if self.reuse_path and self.model_path in os.listdir(os.getcwd()):\n",
    "            print(\"Reusing model trained with same configurations from earlier...\")\n",
    "        else:\n",
    "            print(\"Final training process...\")\n",
    "            train(feat_extractor, cv_data, dev_data, optimizer_options[self.oi], task2 = self.task2, \n",
    "                saveloc=self.model_path, for_crossval=False)\n",
    "        \n",
    "    def final_eval(self):\n",
    "        print(\"loading...\")\n",
    "        model = torch.load(self.model_path) if torch.cuda.is_available() \\\n",
    "            else torch.load(self.model_path, map_location='cpu')        \n",
    "\n",
    "        print(\"final evaluation on held-out test set...\")\n",
    "        print(\"f = \"+ str(\n",
    "              final_eval_f(model, [], self.feat_extractor, predict_batch_size = 64, task2=self.task2, test=True) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning task 1\n"
     ]
    }
   ],
   "source": [
    "task1_sim = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting cross validation optimizer and feature threshold...\n",
      "New feature extractor initialized, but no features yet as no valid path in this directory is specified.\n",
      "\t-- please submit an X set and if necessary a threshhold to build new features\n",
      "Determining word shape features... (uft = 20)\n",
      "On 50000th description...\n",
      "On 100000th description...\n",
      "On 150000th description...\n",
      "On 200000th description...\n",
      "On 250000th description...\n",
      "On 300000th description...\n",
      "On 350000th description...\n",
      "On 400000th description...\n",
      "On 450000th description...\n",
      "On 500000th description...\n",
      "On 550000th description...\n",
      "On 600000th description...\n",
      "On 650000th description...\n",
      "On 700000th description...\n",
      "On 750000th description...\n",
      "On 800000th description...\n",
      "Determining unigram features\n",
      "On 50000th description...\n",
      "On 100000th description...\n",
      "On 150000th description...\n",
      "On 200000th description...\n",
      "On 250000th description...\n",
      "On 300000th description...\n",
      "On 350000th description...\n",
      "On 400000th description...\n",
      "On 450000th description...\n",
      "On 500000th description...\n",
      "On 550000th description...\n",
      "On 600000th description...\n",
      "On 650000th description...\n",
      "On 700000th description...\n"
     ]
    }
   ],
   "source": [
    "task1_sim.do_cv(cv_folds=1, ignore_bigrams =True) # just 1 fold and ignore bigrams to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_sim.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_sim.final_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_sim.final_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_sim = Experiment(task2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_sim.do_cv(cv_folds=1,ignore_bigrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_sim.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
